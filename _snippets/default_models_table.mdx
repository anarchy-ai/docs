| Model Name                          | Developer                                        | Model Size | Description                             |
|-------------------------------------|--------------------------------------------------|-------------------|-----------------------------------------|
| GPT-4                               | [OpenAI](https://platform.openai.com/)           | 800 GB     | High performance, versatile model for advanced chatbot applications             |
| GPT-4-1106-preview                  | [OpenAI](https://platform.openai.com/)           | 1.2 TB     | Latest version with cutting-edge features and improvements                     |
| GPT-4-turbo                         | [OpenAI](https://platform.openai.com/)           | 1.5 TB     | Fastest response times, optimized for speed and efficiency                     |
| GPT-4o                              | [OpenAI](https://platform.openai.com/)           | 1.1 TB     | Optimized for specific tasks            |
| GPT-3.5-turbo                       | [OpenAI](https://platform.openai.com/)           | 350 GB     | Cost-effective, fast responses, suitable for general-purpose chatbots          |
| RedPajama-INCITE-7B-Chat            | [TogetherAI](https://together.ai)                | 1.12 GB    | Lightweight, efficient model designed for chat applications                   |
| Llama-2-7B-32K-Instruct             | [Meta, TogetherAI](https://huggingface.co/meta-llama) | 308 MB     | Handles extended context, ideal for long conversations                         |
| Mistral-7B-Instruct-v0.1            | [MistralAI](https://huggingface.co/mistralai)    | 6.85 GB    | High accuracy, instruction-tuned for better task following                    |
| Mistral-7B-Instruct-v0.2            | [MistralAI](https://huggingface.co/mistralai)    | 5.31 GB    | Enhanced instruction-following capabilities for improved interaction quality   |
| OpenHermes-2p5-Mistral-7B           | [MistralAI](https://huggingface.co/mistralai)    | 166 MB     | Hybrid model, excels in handling complex and diverse tasks                    |
| Nous-Hermes-2-Mixtral-8x7B-DPO      | [Mixtral](https://huggingface.co/Mixtral)       | 662 MB     | Optimized diverse parameters for versatile chatbot applications               |
| Mixtral-8x7B-Instruct-v0.1          | [Mixtral](https://huggingface.co/Mixtral)     | 166 MB     | Instruction-tuned, efficient for specific tasks with high precision            |
| Nous-Hermes-2-Mixtral-8x7B-SFT      | [Mixtral](https://huggingface.co/Mixtral)     | 166 MB     | Supervised fine-tuning ensures high precision and accuracy in responses       |
