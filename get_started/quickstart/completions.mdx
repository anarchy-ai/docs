---
title: üìù Generating Completions
description: "You can quickly start generating completions through OpenAI or locally with the LLM-VM in 3 lines of code. Just specify a ```big_model``` you can select where your completions are generated from."
---

## Examples
Here we give two examples of how you can generate completions with our LLM-VM.
- ```OpenAI Endpoint``` calls OpenAI's [gpt-3.5-turbo](https://platform.openai.com/docs/models/gpt-3-5) model for a completion, which requires your OpenAI API Key and utilizes their endpoint.
- ```Local Endpoint``` example shows you how you can locally use an LLM to generate completions just as easily.

<CodeGroup>
```python OpenAI Endpoint
# import our client
from llm_vm.client import Client

# Selecting the Chat GPT endpoint from OpenAI 
client=Client(big_model='chat_gpt')

# Put in your prompt and go!
response=client.complete(
	prompt='What is Anarchy?',
	context='',
	openai_key='OPENAI_API_KEY')
print(response)
# Anarchy is a political ideology that advocates for the absence of government...
```

```python Local Endpoint
# import our client
from llm_vm.client import Client

# Selecting the Bloom LLM for generation
client=Client(big_model='bloom')

# Put in your prompt and go!
response=client.complete(
	prompt='What is Anarchy?',
	context='') # OPENAI_API_KEY only needed for OpenAI calls
print(response)
# Anarchy is a political ideology that advocates for the absence of government...
```

</CodeGroup>

<Warning>Using OpenAI's models requires an OpenAI API Key and may result in costs not associated with Anarchy's LLM-VM</Warning>

## Supported Models
We support several open LLM model families. You can see which ones and the default models used below.

<Accordion title="Supported Models (and sizes)">
<Snippet file="default_models_table.mdx" />
Just replace ```'chat_gpt'``` in the example with your desired model and you're all set.
</Accordion>
For more information on selecting models visit our [Local LLMs](local_llms) section.

<Snippet file="github.mdx" />
